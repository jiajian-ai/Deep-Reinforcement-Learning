# DQN (Deep Q-Network) ç®—æ³•è¯¦è§£

## ğŸ“š ä»€ä¹ˆæ˜¯DQNï¼Ÿ

DQNï¼ˆDeep Q-Networkï¼Œæ·±åº¦Qç½‘ç»œï¼‰æ˜¯ç”±DeepMindåœ¨2013å¹´æå‡ºçš„çªç ´æ€§ç®—æ³•ï¼Œé¦–æ¬¡å°†æ·±åº¦å­¦ä¹ æˆåŠŸåº”ç”¨äºå¼ºåŒ–å­¦ä¹ ã€‚å®ƒèƒ½å¤Ÿç›´æ¥ä»é«˜ç»´æ„ŸçŸ¥è¾“å…¥ï¼ˆå¦‚å›¾åƒï¼‰å­¦ä¹ æ§åˆ¶ç­–ç•¥ï¼Œåœ¨Atariæ¸¸æˆä¸­è¾¾åˆ°äº†äººç±»æ°´å¹³ã€‚

## ğŸ¯ æ ¸å¿ƒæ€æƒ³

### åŸºæœ¬åŸç†
DQNç»“åˆäº†Q-Learningå’Œæ·±åº¦ç¥ç»ç½‘ç»œï¼š
1. **ä»·å€¼å­¦ä¹ **ï¼šå­¦ä¹ Q(s,a)å‡½æ•°ï¼Œè¯„ä¼°åœ¨çŠ¶æ€sé‡‡å–åŠ¨ä½œaçš„ä»·å€¼
2. **æ·±åº¦ç½‘ç»œ**ï¼šä½¿ç”¨ç¥ç»ç½‘ç»œè¿‘ä¼¼Qå‡½æ•°
3. **ç»éªŒå›æ”¾**ï¼šå­˜å‚¨å’Œé‡ç”¨å†å²ç»éªŒ
4. **ç›®æ ‡ç½‘ç»œ**ï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹

### Q-Learningå›é¡¾
ä¼ ç»ŸQ-Learningä½¿ç”¨è¡¨æ ¼å­˜å‚¨Qå€¼ï¼š
- **çŠ¶æ€ç©ºé—´å°**ï¼šå¯è¡Œ
- **çŠ¶æ€ç©ºé—´å¤§**ï¼šè¡¨æ ¼çˆ†ç‚¸ï¼Œä¸å¯è¡Œ
- **DQNè§£å†³æ–¹æ¡ˆ**ï¼šç”¨ç¥ç»ç½‘ç»œè¿‘ä¼¼Qå‡½æ•°

## ğŸ”¬ ç®—æ³•åŸç†

### 1. Qå‡½æ•°ä¸Bellmanæ–¹ç¨‹

**Qå‡½æ•°å®šä¹‰**ï¼š
```
Q(s, a) = E[R_t | s_t=s, a_t=a]
```
è¡¨ç¤ºåœ¨çŠ¶æ€sé‡‡å–åŠ¨ä½œaåçš„æœŸæœ›ç´¯ç§¯å›æŠ¥ã€‚

**Bellmanæœ€ä¼˜æ–¹ç¨‹**ï¼š
```
Q*(s, a) = E[r + Î³ Â· max_a' Q*(s', a')]
```

### 2. DQNçš„Qç½‘ç»œ

ä½¿ç”¨æ·±åº¦ç¥ç»ç½‘ç»œè¿‘ä¼¼Qå‡½æ•°ï¼š

```python
class QNetwork(nn.Module):
    def forward(self, state):
        x = relu(fc1(state))
        x = relu(fc2(x))
        q_values = fc3(x)  # è¾“å‡ºæ¯ä¸ªåŠ¨ä½œçš„Qå€¼
        return q_values
```

**è¾“å…¥**ï¼šçŠ¶æ€s  
**è¾“å‡º**ï¼šæ‰€æœ‰åŠ¨ä½œçš„Qå€¼ [Q(s,aâ‚), Q(s,aâ‚‚), ..., Q(s,aâ‚™)]

### 3. ç»éªŒå›æ”¾ï¼ˆExperience Replayï¼‰

**é—®é¢˜**ï¼šè¿ç»­æ ·æœ¬é«˜åº¦ç›¸å…³ï¼Œè¿åç‹¬ç«‹åŒåˆ†å¸ƒå‡è®¾

**è§£å†³æ–¹æ¡ˆ**ï¼šç»éªŒå›æ”¾ç¼“å†²åŒº
```python
class ReplayBuffer:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)
```

**ä¼˜åŠ¿**ï¼š
- æ‰“ç ´æ ·æœ¬ç›¸å…³æ€§
- æé«˜æ ·æœ¬åˆ©ç”¨æ•ˆç‡
- æ›´ç¨³å®šçš„è®­ç»ƒ

### 4. ç›®æ ‡ç½‘ç»œï¼ˆTarget Networkï¼‰

**é—®é¢˜**ï¼šQç½‘ç»œæ›´æ–°æ—¶ï¼Œç›®æ ‡ä¹Ÿåœ¨å˜åŒ–ï¼Œå¯¼è‡´ä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**ï¼šä½¿ç”¨å›ºå®šçš„ç›®æ ‡ç½‘ç»œ
```python
# ä¸»ç½‘ç»œï¼šå®æ—¶æ›´æ–°
q_current = q_net(state)[action]

# ç›®æ ‡ç½‘ç»œï¼šå®šæœŸæ›´æ–°
with torch.no_grad():
    q_target = reward + gamma * target_net(next_state).max()

# æŸå¤±
loss = MSE(q_current, q_target)
```

**æ›´æ–°ç­–ç•¥**ï¼š
```python
# æ¯Næ­¥åŒæ­¥ä¸€æ¬¡
if step % target_update == 0:
    target_net.load_state_dict(q_net.state_dict())
```

### 5. Îµ-è´ªå©ªæ¢ç´¢

**æ¢ç´¢ä¸åˆ©ç”¨çš„å¹³è¡¡**ï¼š
```python
def select_action(state, epsilon):
    if random.random() < epsilon:
        return random_action()  # æ¢ç´¢
    else:
        return argmax(Q(state))  # åˆ©ç”¨
```

**Îµè¡°å‡**ï¼š
```python
epsilon = max(epsilon_end, epsilon * epsilon_decay)
```
- åˆæœŸï¼šé«˜Îµï¼Œå¤šæ¢ç´¢
- åæœŸï¼šä½Îµï¼Œå¤šåˆ©ç”¨

## ğŸ’» ä»£ç å®ç°è¦ç‚¹

### 1. è®­ç»ƒå¾ªç¯
```python
for episode in range(num_episodes):
    state = env.reset()
    
    while not done:
        # é€‰æ‹©åŠ¨ä½œï¼ˆÎµ-è´ªå©ªï¼‰
        action = select_action(state, epsilon)
        
        # æ‰§è¡ŒåŠ¨ä½œ
        next_state, reward, done = env.step(action)
        
        # å­˜å‚¨ç»éªŒ
        replay_buffer.push(state, action, reward, next_state, done)
        
        # ä»ç¼“å†²åŒºé‡‡æ ·å¹¶æ›´æ–°
        if len(replay_buffer) > batch_size:
            batch = replay_buffer.sample(batch_size)
            update_q_network(batch)
        
        state = next_state
    
    # è¡°å‡epsilon
    epsilon = max(epsilon_end, epsilon * epsilon_decay)
    
    # æ›´æ–°ç›®æ ‡ç½‘ç»œ
    if episode % target_update == 0:
        target_net.load_state_dict(q_net.state_dict())
```

### 2. Qç½‘ç»œæ›´æ–°
```python
def update_q_network(batch):
    states, actions, rewards, next_states, dones = batch
    
    # å½“å‰Qå€¼
    current_q = q_net(states).gather(1, actions)
    
    # ç›®æ ‡Qå€¼
    with torch.no_grad():
        next_q = target_net(next_states).max(1)[0]
        target_q = rewards + (1 - dones) * gamma * next_q
    
    # è®¡ç®—æŸå¤±
    loss = F.mse_loss(current_q, target_q)
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

### 3. åŠ¨ä½œé€‰æ‹©
```python
def select_action(state, epsilon=0.1):
    if random.random() < epsilon:
        # éšæœºæ¢ç´¢
        return env.action_space.sample()
    else:
        # è´ªå©ªé€‰æ‹©
        with torch.no_grad():
            q_values = q_net(state)
            return q_values.argmax().item()
```

## ğŸ”‘ å…³é”®è¶…å‚æ•°

### 1. learning_rate (å­¦ä¹ ç‡)
- **ä½œç”¨**: æ§åˆ¶Qç½‘ç»œæ›´æ–°é€Ÿåº¦
- **å…¸å‹å€¼**: 0.0001 - 0.001
- **å»ºè®®**: æ¯”ç­–ç•¥æ¢¯åº¦æ–¹æ³•æ›´å°

### 2. gamma (æŠ˜æ‰£å› å­)
- **ä½œç”¨**: å¹³è¡¡å³æ—¶å’Œé•¿æœŸå¥–åŠ±
- **å…¸å‹å€¼**: 0.95 - 0.99
- **CartPole**: 0.99ï¼ˆé‡è§†é•¿æœŸç¨³å®šï¼‰

### 3. epsilon (æ¢ç´¢ç‡)
- **epsilon_start**: 1.0ï¼ˆåˆå§‹å…¨æ¢ç´¢ï¼‰
- **epsilon_end**: 0.01ï¼ˆæœ€ç»ˆä¿ç•™1%æ¢ç´¢ï¼‰
- **epsilon_decay**: 0.995ï¼ˆè¡°å‡é€Ÿç‡ï¼‰

### 4. batch_size (æ‰¹æ¬¡å¤§å°)
- **ä½œç”¨**: æ¯æ¬¡æ›´æ–°ä½¿ç”¨çš„æ ·æœ¬æ•°
- **å…¸å‹å€¼**: 32 - 128
- **æƒè¡¡**: å¤§æ‰¹æ¬¡æ›´ç¨³å®šï¼Œå°æ‰¹æ¬¡æ›´æ–°æ›´é¢‘ç¹

### 5. buffer_capacity (ç¼“å†²åŒºå¤§å°)
- **ä½œç”¨**: ç»éªŒå›æ”¾ç¼“å†²åŒºå®¹é‡
- **å…¸å‹å€¼**: 10000 - 100000
- **æƒè¡¡**: å¤§ç¼“å†²åŒºæ›´diverseï¼Œä½†å†…å­˜å ç”¨é«˜

### 6. target_update (ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘ç‡)
- **ä½œç”¨**: å¤šå°‘æ­¥æ›´æ–°ä¸€æ¬¡ç›®æ ‡ç½‘ç»œ
- **å…¸å‹å€¼**: 10 - 100
- **æƒè¡¡**: é¢‘ç¹æ›´æ–°å¿«ä½†ä¸ç¨³å®šï¼Œå°‘æ›´æ–°ç¨³å®šä½†æ…¢

## ğŸ“Š DQN vs å…¶ä»–ç®—æ³•

| ç®—æ³• | æ ·æœ¬æ•ˆç‡ | è®­ç»ƒç¨³å®šæ€§ | åŠ¨ä½œç©ºé—´ | å®ç°éš¾åº¦ |
|------|---------|-----------|---------|---------|
| DQN  | é«˜ | é«˜ | ç¦»æ•£ | ä¸­ç­‰ |
| REINFORCE | ä½ | ä½ | ç¦»æ•£/è¿ç»­ | ç®€å• |
| PPO  | é«˜ | é«˜ | ç¦»æ•£/è¿ç»­ | å¤æ‚ |
| DDPG | é«˜ | ä¸­ | è¿ç»­ | å¤æ‚ |

## ğŸ“ DQNçš„ä¼˜åŠ¿ä¸åŠ£åŠ¿

### ä¼˜åŠ¿
1. **é«˜æ ·æœ¬æ•ˆç‡**: ç»éªŒå›æ”¾ä½¿æ•°æ®é‡ç”¨
2. **è®­ç»ƒç¨³å®š**: ç›®æ ‡ç½‘ç»œå‡å°‘éœ‡è¡
3. **æ˜“äºè°ƒå‚**: ç›¸å¯¹ç¨³å®šï¼Œå‚æ•°æ•æ„Ÿæ€§ä½
4. **off-policy**: å¯ä»¥ä»ä»»ä½•ç­–ç•¥æ”¶é›†çš„æ•°æ®å­¦ä¹ 
5. **é€‚åˆç¦»æ•£åŠ¨ä½œ**: å¤©ç„¶å¤„ç†ç¦»æ•£åŠ¨ä½œç©ºé—´

### åŠ£åŠ¿
1. **ä»…é™ç¦»æ•£åŠ¨ä½œ**: æ— æ³•ç›´æ¥ç”¨äºè¿ç»­åŠ¨ä½œç©ºé—´
2. **è¿‡ä¼°è®¡é—®é¢˜**: Qå€¼å®¹æ˜“è¢«é«˜ä¼°ï¼ˆmaxæ“ä½œï¼‰
3. **å†…å­˜å ç”¨**: éœ€è¦å­˜å‚¨å¤§é‡ç»éªŒ
4. **æ”¶æ•›æ…¢**: ç›¸æ¯”ç­–ç•¥æ¢¯åº¦æ–¹æ³•å¯èƒ½æ›´æ…¢
5. **æ¢ç´¢é—®é¢˜**: Îµ-è´ªå©ªæ¢ç´¢æ•ˆç‡ä¸é«˜

## ğŸš€ DQNçš„åº”ç”¨

### 1. Atariæ¸¸æˆ
- åŸå§‹è®ºæ–‡ä¸­çš„æˆåŠŸæ¡ˆä¾‹
- Breakout, Pong, Space Invadersç­‰
- ç›´æ¥ä»åƒç´ å­¦ä¹ 

### 2. ç»å…¸æ§åˆ¶
- CartPoleï¼ˆæœ¬é¡¹ç›®ï¼‰
- MountainCar
- LunarLander

### 3. æœºå™¨äººæ§åˆ¶
- ç¦»æ•£åŒ–çš„åŠ¨ä½œç©ºé—´
- è·¯å¾„è§„åˆ’
- æŠ“å–ä»»åŠ¡

### 4. æ¨èç³»ç»Ÿ
- ç¦»æ•£çš„æ¨èé€‰é¡¹
- ç”¨æˆ·äº¤äº’ä¼˜åŒ–

## ğŸ“ˆ è®­ç»ƒæŠ€å·§

### 1. å¥–åŠ±è£å‰ª
```python
# é™åˆ¶å¥–åŠ±èŒƒå›´ï¼Œæé«˜ç¨³å®šæ€§
reward = np.clip(reward, -1, 1)
```

### 2. æ¢¯åº¦è£å‰ª
```python
# é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
torch.nn.utils.clip_grad_norm_(q_net.parameters(), max_norm=10)
```

### 3. HuberæŸå¤±
```python
# æ¯”MSEæ›´é²æ£’
loss = F.smooth_l1_loss(current_q, target_q)
```

### 4. ä¼˜å…ˆç»éªŒå›æ”¾ï¼ˆPERï¼‰
```python
# ä¼˜å…ˆé‡‡æ ·é‡è¦çš„ç»éªŒ
priority = abs(td_error) + epsilon
sample_prob = priority^alpha / sum(priority^alpha)
```

### 5. åŒQå­¦ä¹ ï¼ˆDouble DQNï¼‰
```python
# å‡å°‘è¿‡ä¼°è®¡
# ç”¨ä¸»ç½‘ç»œé€‰æ‹©åŠ¨ä½œ
best_action = q_net(next_state).argmax()
# ç”¨ç›®æ ‡ç½‘ç»œè¯„ä¼°åŠ¨ä½œ
target_q = target_net(next_state)[best_action]
```

## ğŸ”¬ å®éªŒå»ºè®®

### åˆæ¬¡å°è¯•
```python
agent = DQN(
    gamma=0.99,
    lr=0.001,
    epsilon_start=1.0,
    epsilon_end=0.01,
    epsilon_decay=0.995,
    batch_size=64,
    buffer_capacity=10000,
    target_update=10
)
```

### å¦‚æœè®­ç»ƒä¸ç¨³å®š
- é™ä½å­¦ä¹ ç‡ (0.0001)
- å¢åŠ ç›®æ ‡ç½‘ç»œæ›´æ–°é—´éš” (20-50)
- ä½¿ç”¨HuberæŸå¤±
- å¢åŠ æ‰¹æ¬¡å¤§å° (128)
- æ·»åŠ æ¢¯åº¦è£å‰ª

### å¦‚æœæ”¶æ•›å¤ªæ…¢
- å¢åŠ å­¦ä¹ ç‡ (0.005)
- å‡å°æ‰¹æ¬¡å¤§å° (32)
- å‡å°epsilonè¡°å‡ç‡ï¼ˆæ›´å¿«è½¬å‘åˆ©ç”¨ï¼‰
- å¢åŠ ç¼“å†²åŒºå¤§å°
- ä½¿ç”¨ä¼˜å…ˆç»éªŒå›æ”¾

### å¦‚æœè¿‡æ‹Ÿåˆ/æ¬ æ¢ç´¢
- å‡æ…¢epsilonè¡°å‡
- å¢åŠ epsilon_end (0.05-0.1)
- ä½¿ç”¨entropy bonus
- æ·»åŠ å™ªå£°ç½‘ç»œï¼ˆNoisyNetï¼‰

## ğŸ’¡ DQNçš„å˜ä½“

### 1. Double DQN (DDQN)
**é—®é¢˜**: Qå€¼è¿‡ä¼°è®¡  
**è§£å†³**: è§£è€¦åŠ¨ä½œé€‰æ‹©å’Œè¯„ä¼°
```python
action = q_net(next_state).argmax()
target_q = target_net(next_state)[action]
```

### 2. Dueling DQN
**åˆ›æ–°**: åˆ†ç¦»çŠ¶æ€ä»·å€¼V(s)å’Œä¼˜åŠ¿å‡½æ•°A(s,a)
```python
Q(s,a) = V(s) + (A(s,a) - mean(A(s)))
```

### 3. Prioritized Experience Replay (PER)
**åˆ›æ–°**: ä¼˜å…ˆé‡‡æ ·TDè¯¯å·®å¤§çš„ç»éªŒ
```python
priority = |td_error| + Îµ
```

### 4. Rainbow DQN
**é›†å¤§æˆ**: ç»“åˆå¤šç§æ”¹è¿›
- Double DQN
- Dueling DQN
- Prioritized Replay
- Multi-step Learning
- Distributional RL
- Noisy Networks

### 5. C51 (Categorical DQN)
**åˆ›æ–°**: å­¦ä¹ Qå€¼åˆ†å¸ƒè€ŒéæœŸæœ›
```python
# è¾“å‡ºQå€¼çš„æ¦‚ç‡åˆ†å¸ƒ
Q(s,a) = support Ã— softmax(logits)
```

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. Mnih, V., et al. (2013). "Playing Atari with Deep Reinforcement Learning"
2. Mnih, V., et al. (2015). "Human-level control through deep reinforcement learning" (Nature)
3. Van Hasselt, H., et al. (2016). "Deep Reinforcement Learning with Double Q-learning"
4. Wang, Z., et al. (2016). "Dueling Network Architectures for Deep Reinforcement Learning"
5. Schaul, T., et al. (2016). "Prioritized Experience Replay"

## ğŸ¯ å­¦ä¹ è·¯å¾„

1. **ç†è§£Q-Learning**: æŒæ¡è¡¨æ ¼Q-LearningåŸºç¡€
2. **ç†è§£æ·±åº¦å­¦ä¹ **: ç¥ç»ç½‘ç»œã€åå‘ä¼ æ’­
3. **ç†è§£ç»éªŒå›æ”¾**: ä¸ºä»€ä¹ˆéœ€è¦æ‰“ç ´ç›¸å…³æ€§
4. **è¿è¡ŒåŸºç¡€DQN**: è¿è¡Œ `dqn_cartpole.py`
5. **å¯è§†åŒ–Qå€¼**: è§‚å¯ŸQå€¼å¦‚ä½•æ”¶æ•›
6. **å®ç°æ”¹è¿›**: å°è¯•Double DQNã€Dueling DQN
7. **å¯¹æ¯”å®éªŒ**: ä¸REINFORCEã€PPOå¯¹æ¯”æ€§èƒ½
8. **æŒ‘æˆ˜Atari**: å°è¯•åœ¨Atariæ¸¸æˆä¸Šåº”ç”¨

## ğŸ› ï¸ è°ƒè¯•æŠ€å·§

### 1. ç›‘æ§æŒ‡æ ‡
```python
# è®°å½•å…³é”®æŒ‡æ ‡
metrics = {
    'avg_q_value': q_values.mean(),
    'loss': loss.item(),
    'epsilon': epsilon,
    'buffer_size': len(replay_buffer),
    'avg_reward': np.mean(recent_rewards)
}
```

### 2. æ£€æŸ¥Qå€¼
```python
# Qå€¼åº”è¯¥é€æ¸å¢é•¿å¹¶ç¨³å®š
print(f"Q values: {q_net(state)}")
```

### 3. å¯è§†åŒ–å­¦ä¹ æ›²çº¿
- å¥–åŠ±æ›²çº¿åº”è¯¥ä¸Šå‡
- æŸå¤±åº”è¯¥ä¸‹é™å¹¶ç¨³å®š
- Qå€¼åº”è¯¥é€æ¸å¢å¤§

### 4. æ£€æŸ¥æ¢ç´¢
```python
# ç¡®ä¿æœ‰è¶³å¤Ÿæ¢ç´¢
exploration_ratio = num_random_actions / total_actions
print(f"Exploration: {exploration_ratio:.2%}")
```

## ğŸ’¡ æ€»ç»“

DQNæ˜¯æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„**é‡Œç¨‹ç¢‘**ï¼Œå®ƒé¦–æ¬¡è¯æ˜äº†æ·±åº¦å­¦ä¹ åœ¨RLä¸­çš„å·¨å¤§æ½œåŠ›ã€‚è™½ç„¶ç°åœ¨æœ‰æ›´å…ˆè¿›çš„ç®—æ³•ï¼Œä½†DQNçš„æ ¸å¿ƒæ€æƒ³ï¼ˆç»éªŒå›æ”¾ã€ç›®æ ‡ç½‘ç»œï¼‰ä»ç„¶å¹¿æ³›åº”ç”¨ã€‚

**æ ¸å¿ƒè®°å¿†ç‚¹**:
- Qå‡½æ•°è¿‘ä¼¼ï¼šç”¨ç¥ç»ç½‘ç»œä¼°è®¡Q(s,a)
- ç»éªŒå›æ”¾ï¼šæ‰“ç ´æ ·æœ¬ç›¸å…³æ€§
- ç›®æ ‡ç½‘ç»œï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹
- Îµ-è´ªå©ªï¼šæ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡

**DQNçš„å†å²åœ°ä½**:
- 2013å¹´ï¼šé¦–æ¬¡æå‡º
- 2015å¹´ï¼šNatureè®ºæ–‡ï¼Œè¾¾åˆ°äººç±»æ°´å¹³
- å¥ å®šäº†æ·±åº¦RLçš„åŸºç¡€
- å¯å‘äº†ä¸€ç³»åˆ—åç»­ç ”ç©¶

**ä½•æ—¶ä½¿ç”¨DQN**:
- âœ… ç¦»æ•£åŠ¨ä½œç©ºé—´
- âœ… éœ€è¦é«˜æ ·æœ¬æ•ˆç‡
- âœ… è¿½æ±‚è®­ç»ƒç¨³å®šæ€§
- âŒ è¿ç»­åŠ¨ä½œç©ºé—´ï¼ˆç”¨DDPG/TD3ï¼‰
- âŒ éœ€è¦on-policyï¼ˆç”¨PPOï¼‰

